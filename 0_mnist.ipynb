{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 0. MNIST with softmax and 2 layer shallow CNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (55000, 784) (55000, 10)\n",
      "validation:  (5000, 784) (5000, 10)\n",
      "test:  (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \", mnist.train.images.shape, mnist.train.labels.shape)\n",
    "print(\"validation: \", mnist.validation.images.shape, mnist.validation.labels.shape)\n",
    "print(\"test: \", mnist.test.images.shape, mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no8yGccjjLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdaee438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_array = mnist.train.images[42,:].reshape(28, -1)\n",
    "label = np.argmax(mnist.train.labels[42])\n",
    "\n",
    "plt.imshow(image_array, cmap = \"gray\")\n",
    "plt.title(\"label = \" + str(label)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** softmax ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros((784, 10)))\n",
    "b = tf.Variable(tf.zeros((10)))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {x: batch_xs, y_ : batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax accuracy: 0.916900\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"softmax accuracy: %f\" % sess.run(accuracy, feed_dict = {x:mnist.test.images, y_ : mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** 2 layer shallow CNN ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_conv, labels = y_))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy: 0.08\n",
      "step 100, training accuracy: 0.8\n",
      "step 200, training accuracy: 0.94\n",
      "step 300, training accuracy: 0.92\n",
      "step 400, training accuracy: 0.94\n",
      "step 500, training accuracy: 0.92\n",
      "step 600, training accuracy: 0.96\n",
      "step 700, training accuracy: 0.94\n",
      "step 800, training accuracy: 0.94\n",
      "step 900, training accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {x: batch[0], y_ : batch[1], keep_prob : 1.0})\n",
    "        print(\"step %d, training accuracy: %g\" % (i, train_accuracy))\n",
    "        \n",
    "    train_step.run(feed_dict = {x:batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    \n",
    "print(\"test accuracy %g\" % accuracy.eval(feed_dict = {x:mnist.test.images, y_ :mnist.test.labels, keep_prob : 1.0}))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
